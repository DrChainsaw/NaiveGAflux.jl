# NaiveGAflux

[![Build Status](https://travis-ci.com/DrChainsaw/NaiveGAflux.jl.svg?branch=master)](https://travis-ci.com/DrChainsaw/NaiveGAflux.jl)
[![Build Status](https://ci.appveyor.com/api/projects/status/github/DrChainsaw/NaiveGAflux.jl?svg=true)](https://ci.appveyor.com/project/DrChainsaw/NaiveGAflux-jl)
[![Codecov](https://codecov.io/gh/DrChainsaw/NaiveGAflux.jl/branch/master/graph/badge.svg)](https://codecov.io/gh/DrChainsaw/NaiveGAflux.jl)

Neural architecture search for [Flux](https://github.com/FluxML/Flux.jl) through a genetic algorithm.

A marketing person might describe it as "practical proxyless NAS using an unrestricted search space".

The more honest purpose is to serve as a pipe cleaner and example for [NaiveNASflux](https://github.com/DrChainsaw/NaiveNASflux.jl) which is doing most of the heavy lifting.

## Basic Usage

```julia
Pkg.add("https://github.com/DrChainsaw/NaiveGAflux.jl")
```

Package is in development. Not ready for use!

### Search Spaces

The search space is a set of possible architectures which the search policy may use to create initial candidates or to extend existing candidates. Search spaces are constructed from simple components which gives a lot of flexibility.

Lets start with the most simple search space, a `ParSpace`:

```julia
# Set seed of default random number generator for reproduceable results
using NaiveGAflux, Random
Random.seed!(NaiveGAflux.rng_default, 123)

ps1d = ParSpace([2,4,6,10])

# Draw from the search space
@test ps1d() == 2
@test ps1d() == 6

# Possible to supply another rng than the default one
@test ps1d(MersenneTwister(0)) == 2

# Can be of any dimension and type
ps2d = ParSpace(["1","2","3"], ["4","5","6","7"])

@test typeof(ps1d) == ParSpace{1, Int64}
@test typeof(ps2d) == ParSpace{2, String}

@test ps2d() == ("3", "6")
```

Lets have a look at an example of a search space for convolutional layers:

```julia
Random.seed!(NaiveGAflux.rng_default, 1)

outsizes = 4:32
kernelsizes = 3:9
cs = ConvSpace2D(BaseLayerSpace(outsizes, [relu, elu, selu]), kernelsizes)

@test typeof(cs) == ConvSpace{2}

inputsize = 16
convlayer = cs(inputsize)

@test string(convlayer) == "Conv((5, 4), 16=>18, NNlib.elu)"
```

Lastly, lets look at how to construct a search space for ResNeXt like models:

```julia
Random.seed!(NaiveGAflux.rng_default, 666)

# VertexSpace creates a MutableVertex of layers generated by the wrapped search space
cs = VertexSpace(ConvSpace2D(BaseLayerSpace(8:256, [identity, relu, elu]), 3:5))
bs = VertexSpace(BatchNormSpace([identity, relu]))

# Block of conv->bn and bn-conv respectively.
# Need to make sure there is always at least one SizeAbsorb layer to make fork and res below play nice
csbs = ListArchSpace(cs ,bs)
bscs = ListArchSpace(bs, cs)

# Randomly generates a conv-block:
cblock = ArchSpace(ParSpace1D(cs, csbs, bscs))

# Generates between 1 and 5 layers from csbs
rep = RepeatArchSpace(cblock, 1:5)

# Generates between 2 and 4 parallel paths joined by concatenation (inception like-blocks) from rep
fork = ForkArchSpace(rep, 2:4)

# Generates a residual connection around what is generated by rep
res = ResidualArchSpace(rep)

# ... and a residual fork
resfork = ResidualArchSpace(fork)

# Pick one of the above randomly...
repforkres = ArchSpace(ParSpace1D(rep, fork, res, resfork))

# ...1 to 3 times
blocks = RepeatArchSpace(repforkres, 1:3)

# End each block with subsamping through maxpooling
ms = VertexSpace(MaxPoolSpace(PoolSpace2D([2])))
reduction = ListArchSpace(blocks, ms)

# And lets do 2 to 4 reductions
featureextract = RepeatArchSpace(reduction, 2:4)

# Adds 1 to 3 dense layers as outputs
dense = VertexSpace(DenseSpace(16:512, [relu, selu]))
drep = RepeatArchSpace(dense, 0:2)
# Last layer has fixed output size depending on number of labels
dout=VertexSpace(DenseSpace(10, identity))
output = ListArchSpace(drep, dout)

# Aaaand lets glue it together: Feature extracting conv+bn layers -> global pooling -> dense layers
archspace = ListArchSpace(featureextract, GpVertex2D(), output)

# Input is 3 channel image
inputshape = inputvertex("input", 3, FluxConv{2}())

# Sample one architecture from the search space
graph1 = CompGraph(inputshape, archspace(inputshape))
@test nv(graph1) == 123

# And one more...
graph2 = CompGraph(inputshape, archspace(inputshape))
@test nv(graph2) == 75
```

### Mutation

Mutation is the way one existing candidate is transformed to a slightly different candidate. NaiveGAflux supports doing this while preserving weights and connections between neurons, thus reducing the impact of mutating an already trained candidate.

The following basic mutation operations are currently supported:
1. Change the output size of vertices using `NoutMutation`
2. Remove vertices using `RemoveVertexMutation`
3. Add vertices using `AddVertexMutation`

To be added:
1. Mutation of kernel size for conv layers

Possible but not planned:
1. Add an edge from one vertex to another
2. Remove and edge from one vertex to another

In addition to the basic mutation operations, there are numerous utilities for adding behaviour and convenience. Here are a few examples:

```julia
using Random
Random.seed!(NaiveGAflux.rng_default, 0)

invertex = inputvertex("in", 3, FluxDense())
layer1 = mutable(Dense(nout(invertex), 4), invertex)
layer2 = mutable(Dense(nout(layer1), 5), layer1)
graph = CompGraph(invertex, layer2)

mutation = NoutMutation(-0.5, 0.5)

@test nout(layer2) == 5

mutation(layer2)

@test nout(layer2) == 6

# VertexMutation applies the wrapped mutation to all vertices in a CompGraph
mutation = VertexMutation(mutation)

@test nout.(vertices(graph)) == [3,4,6]

mutation(graph)

# Input vertex is never mutated
@test nout.(vertices(graph)) == [3,5,4]

# Use the MutationShield trait to protect vertices from mutation
outlayer = mutable(Dense(nout(layer2), 10), layer2, traitfun = MutationShield)
graph = CompGraph(invertex, outlayer)

mutation(graph)

@test nout.(vertices(graph)) == [3,4,3,10]

# In most cases it makes sense to mutate with a certain probability
mutation = VertexMutation(MutationProbability(NoutMutation(-0.5, 0.5), Probability(0.05)))

mutation(graph)

@test nout.(vertices(graph)) == [3,4,2,10]

# Or just chose to either mutate the whole graph or don't do anything
mutation = MutationProbability(VertexMutation(NoutMutation(-0.5, 0.5)), Probability(0.05))

mutation(graph)

@test nout.(vertices(graph)) == [3,4,2,10]

# Up until now, size changes have only been kept track of, but not actually applied
@test nout_org.(vertices(graph)) == [3,4,5,10]

Î”outputs(graph, v -> ones(nout_org(v)))
apply_mutation(graph)

@test nout_org.(vertices(graph)) == [3,4,2,10]
@test size(graph(ones(3,1))) == (10, 1)

# NeuronSelectMutation keeps track of changed vertices and performs the above steps when invoked
mutation = VertexMutation(NeuronSelectMutation(NoutMutation(-0.5,0.5)))

mutation(graph)

@test nout.(vertices(graph)) == [3,5,3,10]
@test nout_org.(vertices(graph)) == [3,4,2,10]

select(mutation.m)

@test nout.(vertices(graph)) == nout_org.(vertices(graph)) == [3,4,2,10]
@test size(graph(ones(3,1))) == (10, 1)

# Mutation can also be conditioned:
mutation = VertexMutation(MutationFilter(v -> nout(v) < 4, RemoveVertexMutation()))

mutation(graph)

@test nout.(vertices(graph)) == [3,5,10]

# Chaining mutations is also useful:
addmut = AddVertexMutation(VertexSpace(DenseSpace(5, identity)))
noutmut = NeuronSelectMutation(NoutMutation(-0.8, 0.8))
mutation = VertexMutation(MutationList(addmut, noutmut))

# For deeply composed blobs like this, it can be cumbersome to "dig up" the NeuronSelectMutation.
# NeuronSelect helps finding NeuronSelectMutations in the compositional hierarchy
neuronselect = NeuronSelect()

# PostMutation lets us add actions to perform after a mutation is done
logselect(m, g) = @info "Selecting parameters..."
mutation = PostMutation(mutation, logselect, neuronselect)

@test_logs (:info, "Selecting parameters...") mutation(graph)

@test nout.(vertices(graph)) == nout_org.(vertices(graph)) == [3,6,5,10]
```
